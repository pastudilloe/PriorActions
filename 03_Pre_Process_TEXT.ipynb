{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a169d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os                   # File and directory operations\n",
    "import re                   # Regular expressions for text splitting & pattern matching\n",
    "import json                 # Read/write JSON for storing the “universe” of topics & policies\n",
    "import csv                  # Read CSV metadata mapping report numbers to project info\n",
    "import spacy                # NLP library for tokenization, lemmatization, stop-word removal\n",
    "from typing import Dict     # Type hinting for dictionaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a036fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global constants\n",
    "\n",
    "BASE_PATH     = '/Users/pastudilloe/Library/CloudStorage/Dropbox/01 CONSULTING/WB_PriorActions_Poverty'\n",
    "SUMMARY_RATIO = 0.8        # Fraction of sentences to keep when doing extractive compression\n",
    "MAX_LINES     = 450        # Only read the first 450 lines from each raw report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39d9121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the small English spaCy model, and bump its maximum document length\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")  \n",
    "nlp.max_length = 5_000_000   # Allow processing very large text without truncate errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c30e2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a lookup table (report_map) from the CSV of World Bank docs\n",
    "\n",
    "csv_path = os.path.join(BASE_PATH, 'Documents', 'world_bank_documents_urls.csv')\n",
    "report_map: Dict[str, Dict[str, str]] = {}\n",
    "\n",
    "if os.path.exists(csv_path):\n",
    "    with open(csv_path, newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            rpt = row.get('Report No.', '').strip()            # Unique report identifier\n",
    "            if not rpt:\n",
    "                continue\n",
    "            report_map[rpt] = {\n",
    "                \"Project Name\": row.get('Project Name', '').strip() or \"UNKNOWN\",\n",
    "                \"Link\":         row.get('Link',         '').strip() or \"UNKNOWN\"\n",
    "            }\n",
    "# At this point, report_map maps each Report No. to its project name and URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ce293c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple extractive compressor: keep only the first SUMMARY_RATIO of sentences\n",
    "\n",
    "def compress_text_extractive(text: str, ratio: float = SUMMARY_RATIO) -> str:\n",
    "    \"\"\"\n",
    "    Split text into sentences, then join only the first `ratio` fraction.\n",
    "    \"\"\"\n",
    "    # Split on sentence boundaries (., !, ?)\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
    "    if len(sentences) < 2:\n",
    "        return text  # Too short to compress\n",
    "    keep_n = max(1, int(len(sentences) * ratio))\n",
    "    return ' '.join(sentences[:keep_n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace37b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text: lemmatize, lowercase, and remove stopwords/punctuation\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    doc = nlp(text)\n",
    "    tokens = [\n",
    "        token.lemma_.lower()\n",
    "        for token in doc\n",
    "        if not token.is_stop and not token.is_punct and not token.is_space\n",
    "    ]\n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d24231a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the Operation or Project ID from the raw text using regex\n",
    "\n",
    "def extract_operation_id(text: str) -> str:\n",
    "    label_re = re.compile(\n",
    "        r\"(?:(?:Operation)|(?:Project))\\s*ID(?:\\s*(?:No\\.?|Number))?\\s*[:\\-]?\\s*\"\n",
    "        r\"([A-Za-z0-9]+(?:[ \\t\\-]+[A-Za-z0-9]+)*)\",\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    m = label_re.search(text)\n",
    "    if m:\n",
    "        # Remove any spaces/tabs from the matched ID\n",
    "        return re.sub(r\"[ \\t]+\", \"\", m.group(1))\n",
    "\n",
    "    # Fallback: look for a standalone pattern like “P123456”\n",
    "    fall_re = re.compile(r\"\\bP\\d{6}\\b\", re.IGNORECASE)\n",
    "    fm = fall_re.search(text)\n",
    "    return fm.group(0) if fm else \"P_UNKNOWN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a35624a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the “universe” description file into a nested dict of topics → policy areas → text\n",
    "\n",
    "def parse_universe_text(fp: str) -> Dict[str, Dict[str, str]]:\n",
    "    universe: Dict[str, Dict[str, str]] = {}\n",
    "    current_topic = None\n",
    "    current_policy = None\n",
    "    buffer_lines = []\n",
    "\n",
    "    # Regex for lines starting “Topic X: …” and “Policy Area Y: …”\n",
    "    topic_re  = re.compile(r\"^Topic\\s*\\d*:\\s*(.+)$\", re.IGNORECASE)\n",
    "    policy_re = re.compile(r\"^Policy\\s+Area\\s*\\d+(?:\\.\\d+)*:\\s*(.+)$\", re.IGNORECASE)\n",
    "\n",
    "    with open(fp, 'r', encoding='utf-8', errors='replace') as f:\n",
    "        for raw in f:\n",
    "            line = raw.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            t = topic_re.match(line)\n",
    "            p = policy_re.match(line)\n",
    "\n",
    "            if t:\n",
    "                # When a new topic starts, flush the previous policy\n",
    "                if current_topic and current_policy:\n",
    "                    universe[current_topic][current_policy] = preprocess_text(\" \".join(buffer_lines))\n",
    "                    buffer_lines = []\n",
    "                current_topic = t.group(1).strip()\n",
    "                universe[current_topic] = {}\n",
    "                current_policy = None\n",
    "\n",
    "            elif p:\n",
    "                # When a new policy starts, flush the previous one\n",
    "                if current_topic and current_policy:\n",
    "                    universe[current_topic][current_policy] = preprocess_text(\" \".join(buffer_lines))\n",
    "                    buffer_lines = []\n",
    "                current_policy = p.group(1).strip()\n",
    "\n",
    "            else:\n",
    "                # Accumulate lines under the current policy\n",
    "                buffer_lines.append(line)\n",
    "\n",
    "        # At EOF, flush the last policy section\n",
    "        if current_topic and current_policy:\n",
    "            universe[current_topic][current_policy] = preprocess_text(\" \".join(buffer_lines))\n",
    "\n",
    "    return universe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1b78f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ID_P113638_50149.txt\n",
      "Saved: ID_PE-P106724-LEN-BB_44841.txt\n",
      "Saved: ID_P115145_49236.txt\n",
      "Saved: ID_P117282_52831.txt\n",
      "Saved: ID_PI14822_50217.txt\n",
      "Saved: ID_P111665_51690.txt\n",
      "Saved: ID_BInter-AmericanDevelopmentBank_49545.txt\n",
      "Saved: ID_P122847_60074.txt\n",
      "Saved: ID_P112544_46299.txt\n",
      "Saved: ID_P115874_53119.txt\n",
      "Saved: ID_P112262_53669.txt\n",
      "Saved: ID_P117838_52749.txt\n",
      "Saved: ID_P117000_53494.txt\n",
      "Saved: ID_P_UNKNOWN_.txt\n",
      "Saved: ID_P115816_47224.txt\n",
      "Saved: ID_PO95388_47556.txt\n",
      "Saved: ID_P101230_48467.txt\n",
      "Saved: ID_P117510_53720.txt\n",
      "Saved: ID_P115102_50177.txt\n",
      "Saved: ID_P_UNKNOWN_52748.txt\n",
      "Saved: ID_PE-P114991-LEN-BB_47194.txt\n",
      "Saved: ID_P_UNKNOWN_57608.txt\n",
      "Saved: ID_PO96711_46671.txt\n",
      "Saved: ID_P118636_53240.txt\n",
      "Saved: ID_BY-P115700_50991.txt\n",
      "Saved: ID_P_UNKNOWN_48304.txt\n",
      "Saved: ID_P120534_53917.txt\n",
      "Saved: ID_P_UNKNOWN_48305.txt\n",
      "Saved: ID_P101232_53731.txt\n",
      "Saved: ID_P116557_58499.txt\n",
      "Saved: ID_P115177_47208.txt\n",
      "Saved: ID_P117278_53531.txt\n",
      "Saved: ID_P_UNKNOWN_47008.txt\n",
      "Saved: ID_P117758_51118.txt\n",
      "Saved: ID_P107335_51130.txt\n",
      "Saved: ID_P_UNKNOWN_49971.txt\n",
      "Saved: ID_P121877_56689.txt\n",
      "Saved: ID_P113301Sectors_47223.txt\n",
      "Saved: ID_P107303_46511.txt\n",
      "Saved: ID_P117822_57309.txt\n",
      "Saved: ID_P107741_44611.txt\n",
      "Saved: ID_P_UNKNOWN_49321.txt\n",
      "Saved: ID_P106834_44822.txt\n",
      "Saved: ID_P_UNKNOWN_48605.txt\n",
      "Saved: ID_P120313_54238.txt\n",
      "Saved: ID_P113306_51426.txt\n",
      "Saved: ID_P122157_57527.txt\n",
      "Saved: ID_P106708_51143.txt\n",
      "Saved: ID_P112368Sectors_46167.txt\n",
      "Saved: ID_P115608_49491.txt\n",
      "Saved: ID_ATheInternationalDevelopmentAssociation_51590.txt\n",
      "Saved: ID_P115638_47480.txt\n",
      "Saved: ID_P116984_50883.txt\n",
      "Saved: ID_P102018_48606.txt\n",
      "Saved: ID_P104937_47535.txt\n",
      "Saved: ID_PE-P116264-LEN-BB_55657.txt\n",
      "Saved: ID_P_UNKNOWN_59910.txt\n",
      "Saved: ID_P123374Sectors_57083.txt\n",
      "Saved: ID_P111182_54039.txt\n",
      "Saved: ID_P122202_59253.txt\n",
      "Saved: ID_P121778_56835.txt\n",
      "Saved: ID_P116215_53781.txt\n",
      "Saved: ID_P117610_59252.txt\n",
      "Saved: ID_P117370ClosingDate_58170.txt\n",
      "Saved: ID_P117234_50921.txt\n",
      "Saved: ID_P127331_55656.txt\n",
      "Saved: ID_P120947_57109.txt\n",
      "Saved: ID_P120134_53637.txt\n",
      "Saved: ID_P115659_58010.txt\n",
      "Saved: ID_P121800_57323.txt\n",
      "Saved: ID_P114291Environmentalscreeningcategory_48549.txt\n",
      "Saved: ID_P122667_59493.txt\n",
      "Saved: ID_P_UNKNOWN_50845.txt\n",
      "Saved: ID_P115732_51434.txt\n",
      "Saved: ID_BInter-AmericanDevelopmentforCustoms_46809.txt\n",
      "Saved: ID_P120946_58821.txt\n",
      "Saved: ID_P121178_60373.txt\n",
      "Saved: ID_P125837_59914.txt\n",
      "Saved: ID_P115199_47280.txt\n",
      "Saved: ID_P107398_47519.txt\n",
      "Saved: ID_P_UNKNOWN_47492.txt\n",
      "Saved: ID_P122783_59486.txt\n",
      "Saved: ID_P117667_56012.txt\n",
      "Saved: ID_P118814_53195.txt\n",
      "Saved: ID_P118070_51219.txt\n",
      "Saved: ID_P112227_52670.txt\n",
      "Saved: ID_P112817_52466.txt\n",
      "Saved: ID_P116178_48416.txt\n",
      "Saved: ID_P117723_51146.txt\n",
      "Saved: ID_P117238_53989.txt\n",
      "Saved: ID_P117201_52301.txt\n",
      "Saved: ID_P117270_52467.txt\n",
      "Saved: ID_P_UNKNOWN_48167.txt\n",
      "Saved: ID_P116020_48211.txt\n",
      "Saved: ID_P112369_46219.txt\n",
      "Saved: ID_P108489_43361.txt\n",
      "Saved: ID_P117944_49499.txt\n",
      "Saved: ID_P_UNKNOWN_55649.txt\n",
      "Saved: ID_P_UNKNOWN_47067.txt\n",
      "Saved: ID_P117016_53990.txt\n",
      "Saved: ID_P117698_54972.txt\n",
      "Saved: ID_P115709_49937.txt\n",
      "Saved: ID_P116937_60340.txt\n",
      "Saved: ID_P113235_48396.txt\n",
      "Saved: ID_P120399_54973.txt\n",
      "Saved: ID_P116088_49077.txt\n",
      "Saved: ID_P_UNKNOWN_52669.txt\n",
      "Saved: ID_P101177_47104.txt\n",
      "Saved: ID_PE-P122221_58226.txt\n",
      "Saved: ID_P117421_54352.txt\n",
      "Saved: ID_P108258_59845.txt\n",
      "Saved: ID_PE-P119856-LEN-BB_53575.txt\n",
      "Saved: ID_P110109_48141.txt\n",
      "Saved: ID_P113456_47272.txt\n",
      "Saved: ID_P122640_56425.txt\n",
      "Saved: ID_P_UNKNOWN_51174.txt\n",
      "Saved: ID_P121812_56183.txt\n",
      "Saved: ID_P113069_55893.txt\n",
      "Saved: ID_P125425_59071.txt\n",
      "Saved: ID_P122391_58227.txt\n",
      "Saved: ID_PI14937_52524.txt\n",
      "Saved: ID_P_UNKNOWN_54627.txt\n",
      "Saved: ID_P112264_58431.txt\n",
      "Saved: ID_P_UNKNOWN_46814.txt\n",
      "Saved: ID_P111164_47908.txt\n",
      "Saved: ID_P_UNKNOWN_47712.txt\n",
      "Saved: ID_AM-P116451_57267.txt\n",
      "Saved: ID_P116666_56347.txt\n",
      "Saved: ID_P117281_52647.txt\n",
      "Saved: ID_P115737_48794.txt\n",
      "Saved: ID_P_UNKNOWN_57111.txt\n",
      "Saved: ID_P102651_58222.txt\n",
      "Saved: ID_P122699_59896.txt\n",
      "Saved: ID_P115837_54430.txt\n",
      "Saved: ID_P101471_44351.txt\n",
      "Saved: ID_PI07288ClosingDate_48557.txt\n",
      "Saved: ID_P122807_59328.txt\n",
      "Saved: ID_PI21056_54340.txt\n",
      "Saved: ID_P122796ClosingDate_59843.txt\n",
      "Saved: ID_P114463_52876.txt\n",
      "Saved: ID_P112700_48437.txt\n",
      "Saved: ID_P117692_50251.txt\n",
      "Saved: ID_P113893_51577.txt\n",
      "Saved: ID_P118531_57112.txt\n",
      "Saved: ID_P117229_54355.txt\n",
      "Saved: ID_P117161_54341.txt\n",
      "Saved: ID_P118188_53638.txt\n",
      "Saved: ID_P113450Sectors_46508.txt\n",
      "Saved: ID_P_UNKNOWN_46050.txt\n",
      "Saved: ID_P118931_59170.txt\n",
      "Saved: ID_P116787_52025.txt\n",
      "Saved: ID_P_UNKNOWN_54497.txt\n",
      "Saved: ID_P125114_57565.txt\n",
      "Saved: ID_P_UNKNOWN_45613.txt\n",
      "Saved: ID_P113241_51897.txt\n",
      "Saved: ID_P106083_46455.txt\n",
      "Saved: ID_P110147_54247.txt\n",
      "Saved: ID_P117043_52959.txt\n",
      "Saved: ID_P117203_48083.txt\n",
      "Saved: ID_P117237_54333.txt\n",
      "Saved: ID_P116353_55039.txt\n",
      "Saved: ID_P117924_51920.txt\n",
      "Saved: ID_P_UNKNOWN_54858.txt\n",
      "Saved: ID_P_UNKNOWN_49162.txt\n",
      "Saved: ID_P118036_50382.txt\n",
      "Saved: ID_P123073_59629.txt\n",
      "Saved: ID_P122470_57016.txt\n",
      "Saved: ID_P116125_48644.txt\n",
      "Saved: ID_P107493_46899.txt\n",
      "Saved: ID_P117244_52146.txt\n",
      "Saved: ID_P118713_54913.txt\n",
      "Saved: ID_P115143_49573.txt\n",
      "Saved: ID_P116608ClosingDate_50181.txt\n",
      "Saved: ID_P120564_53488.txt\n",
      "Saved: ID_P107218Sectors_47402.txt\n",
      "Saved: ID_PE-P103770-LEN-BB_51738.txt\n",
      "Saved: ID_P112495_51062.txt\n",
      "Saved: ID_P123255_57417.txt\n",
      "Saved: ID_P115958_49365.txt\n",
      "Saved: ID_P_UNKNOWN_50152.txt\n",
      "Saved: ID_PI06720_50620.txt\n",
      "Saved: ID_P116951_49359.txt\n",
      "Saved: ID_P123267_58297.txt\n",
      "Saved: ID_P102607_47994.txt\n",
      "Saved: ID_P114154_48331.txt\n",
      "Saved: ID_PO95205_47215.txt\n",
      "Saved: ID_P123685_58533.txt\n",
      "Saved: ID_P122483_57600.txt\n",
      "Saved: ID_P115426_57372.txt\n",
      "Saved: ID_P123196_60535.txt\n",
      "Saved: ID_P_UNKNOWN_56291.txt\n",
      "Saved: ID_P113176_47559.txt\n",
      "Saved: ID_P113172_48656.txt\n",
      "Saved: ID_PO99033Sectors_48468.txt\n",
      "Saved: ID_P118239_54732.txt\n",
      "Saved: ID_P113372_46685.txt\n",
      "Saved: ID_P120470_53277.txt\n",
      "Saved: ID_P108759_50193.txt\n",
      "Saved: ID_P117279_54493.txt\n",
      "Saved: ID_P119214_51528.txt\n",
      "Saved: ID_P112612_51064.txt\n",
      "\n",
      "Processed 201 reports.\n"
     ]
    }
   ],
   "source": [
    "# Main execution block\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Paths for universe input, universe output, raw and processed reports\n",
    "    uni_in   = os.path.join(BASE_PATH, \"Helpers\", \"Prior_Actions_DESCRIPTION.txt\")\n",
    "    uni_out  = os.path.join(BASE_PATH, \"Helpers\", \"Prior_Actions_PROCESSED.txt\")\n",
    "    raw_dir  = os.path.join(BASE_PATH, \"Datasets\", \"Raw\", \"policy_reports_test\")\n",
    "    proc_dir = os.path.join(BASE_PATH, \"Datasets\", \"Processed\", \"policy_reports_test\")\n",
    "\n",
    "    # 1) Parse the universe file & write it out as JSON\n",
    "    universe = parse_universe_text(uni_in)\n",
    "    with open(uni_out, 'w', encoding='utf-8') as uf:\n",
    "        json.dump(universe, uf, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # 2) Ensure raw reports exist and create processed directory\n",
    "    if not os.path.exists(raw_dir):\n",
    "        print(f\"Raw folder missing: {raw_dir}\")\n",
    "        exit(1)\n",
    "    os.makedirs(proc_dir, exist_ok=True)\n",
    "\n",
    "    # 3) Loop over each .txt report, process, and save\n",
    "    count = 0\n",
    "    for fn in os.listdir(raw_dir):\n",
    "        if not fn.lower().endswith('.txt'):\n",
    "            continue\n",
    "\n",
    "        # Read up to MAX_LINES from the report\n",
    "        with open(os.path.join(raw_dir, fn), 'r', encoding='utf-8', errors='replace') as rf:\n",
    "            raw_lines = [next(rf, '') for _ in range(MAX_LINES)]\n",
    "        raw_text = \"\".join(raw_lines)\n",
    "\n",
    "        # Extract metadata\n",
    "        op_id  = extract_operation_id(raw_text)\n",
    "        rpt_no = os.path.splitext(fn)[0]\n",
    "        meta   = report_map.get(rpt_no, {\"Project Name\": \"UNKNOWN\", \"Link\": \"UNKNOWN\"})\n",
    "        pname  = meta[\"Project Name\"]\n",
    "        plink  = meta[\"Link\"]\n",
    "\n",
    "        # Compress and preprocess the text\n",
    "        brief     = compress_text_extractive(raw_text)\n",
    "        processed = preprocess_text(brief)\n",
    "\n",
    "        # Build a header with ID, report number, project name, and link\n",
    "        header = (\n",
    "            f\"ID {op_id} {fn}\\n\"\n",
    "            f\"Report: {rpt_no}\\n\"\n",
    "            f\"Project Name: {pname}\\n\"\n",
    "            f\"Link: {plink}\\n\\n\"\n",
    "        )\n",
    "        content = header + processed\n",
    "\n",
    "        # Create a filesystem-safe filename from the header line\n",
    "        safe_name = re.sub(r\"\\s+\", \"_\", header.splitlines()[0].strip())\n",
    "        out_path  = os.path.join(proc_dir, safe_name)\n",
    "\n",
    "        # Write the processed content to disk\n",
    "        with open(out_path, 'w', encoding='utf-8') as wf:\n",
    "            wf.write(content)\n",
    "\n",
    "        print(f\"Saved: {safe_name}\")\n",
    "        count += 1\n",
    "\n",
    "    print(f\"\\nProcessed {count} reports.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3b23b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
